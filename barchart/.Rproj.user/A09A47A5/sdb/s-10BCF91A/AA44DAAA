{
    "contents" : "#-----------------------------------------------------------------------\n#     VERSION HISTORY OF fos.R\n#-----------------------------------------------------------------------\n# Version: v1.0.0\n# Date: 6/16/16\n# Author: Alex Fling\n# Comment: Outlined the fos function.\n\n#-----------------------------------------------------------------------\n#     BEGINNING OF THE CODE\n#-----------------------------------------------------------------------\n\n#####################\n### Required Packages\n## pracma provides the logspace function, and quadrupen provides the elastic.net function\n\nlibrary(pracma)\nlibrary(quadrupen)\n\n####################\n### Helper Functions\n## infNorm, l2NormSq, and l1Norm help make the code more readable\n\n# returns the infinity-norm of the given vector\ninfNorm <- function(x) {\n  return(norm(x, type=c('I')))\n}\n\n# returns the squared l2-norm of the given vector\nl2NormSq <- function(x) {\n  return(norm(x, type=c('F'))^2)\n}\n\n# returns the l1-norm of the given vector\nl1Norm <- function(x) {\n  return(norm(x, type=c('1')))\n}\n\n#soft_thresh <- function(x, a) {\n#  tmp = abs(x) - a\n#  tmp = tmp * (tmp >= 0) * sign(x)\n#  return(tmp)\n#}\n\n################################\n### Adaptive Validation with FOS\n## Returns the Lasso regression fit for the given dataset using Adaptive Validation and FOS.\n## Variable selection can also be applied.\n# x = design matrix (the dataset without the values of the response variable)\n# y = outcome (the values of the selected response variable)\n\nAdaptiveValidationFOS <- function(x, y) {\n\n  # prepare x and y to be used in calculations and set C\n  x = as.matrix(x)\n  y = y[, 1]\n  C = 0.75\n  browser()\n  #################################\n  ### Adaptive Calibration with FOS\n  ## Finds the AV tuning parameter for the dataset.\n  # x = n by p matrix representing the dataset (without the response variable column), where n is\n  #     the number of training examples and p is the number of variables\n  # y = n by 1 vector representing the values of the selected response variable\n  # C = scalar constant\n\n  # useful constants (dimensions of the data)\n  n = nrow(x) # number of samples\n  p = ncol(x) # number of predictors\n\n  # used in the duality gap check\n  gam = 1\n\n  # center and standardize the data (better way to vectorize this?)\n  for (i in 1:p) {\n    x[,i] = (x[,i] - mean(x[,i])) / std(x[,i])\n  }\n  y = (y - mean(y)) / std(y)\n\n  # set up the lambda sequence (rs[1] = rMax > rs[2] > ... > rM > 0)\n  M = 100\n  rMax = 2 * infNorm(t(x) %*% y)\n  rMin = 0.001 * rMax\n  rs = logspace(log10(rMax), log10(rMin), M)\n\n  # initialization for outer loop\n  statsCont = TRUE\n  statsIt = 1\n  betas = matrix(0, nrow=p, ncol=M) # p by M matrix where each column betas[,j] is associated with rs[j]\n  rtilda = rs[M]\n\n  # used for debugging\n  outerloopcount = 0\n\n  # outer loop\n  while (statsCont && statsIt < M) {\n\n    # initialization for inner loop\n    statsIt = statsIt + 1\n    stopCrit = FALSE\n    betaOld = betas[,statsIt - 1]\n    rStatsIt = rs[statsIt]\n\n    # inner loop\n    while (!stopCrit) {\n\n      # values used in duality gap computation\n      beta.t = betas[,statsIt]\n      error = x %*% beta.t - y\n\n      # compute the dual point, nu.t\n      alternative = rStatsIt / infNorm(2 * t(x) %*% error)\n      s = min(\n        max(alternative,\n            (-1 * t(y) %*% error) / l2NormSq(y - x %*% beta.t)),\n        -1 * alternative)\n      nu.t = -1 * (2 * s / rStatsIt) * error\n\n      # compute duality gap\n      f.beta = l2NormSq(error) + rStatsIt * l1Norm(beta.t)\n      D.nu = 0.25 * rStatsIt^2 * l2NormSq(nu.t + (2 / rStatsIt) * y) - l2NormSq(y)\n      duality.gap = f.beta + D.nu\n\n      # update the stopping criterion for the inner loop (stopCrit)\n      if (duality.gap <= gam * C^2 * rStatsIt^2 / n) {\n        betas[,statsIt] = betaOld\n        stopCrit = TRUE\n      } else {\n        # update beta: extract coefficients from dgCMatrix\n        net = elastic.net(x, y, lambda1=(rStatsIt / 2), lambda2=0,\n                          intercept=FALSE, normalize=FALSE,\n                          beta0=betaOld, control=list(method=\"fista\", max.iter=1))\n        betas[,statsIt] = net@coefficients[1,]\n\n        # update betaOld\n        betaOld = betas[,statsIt]\n      }\n    }\n\n    # update the stopping criterion for the outer loop (statsCont)\n    for (k in 1:statsIt) {\n      betak = betas[,k]\n      rk = rs[k]\n      statsCont = statsCont && (n * max(abs(betas[,statsIt] - betak)) / (rStatsIt + rk) <= C)\n    }\n  }\n\n  #if (!statsCont) {\n  #  rtilda = rs[statsIt] unnecessary???\n  #}\n\n  avfosfit = betas[,statsIt]\n\n  ######################\n  ### Variable Selection\n  ## Applies a threshold to each component (i.e., each coefficient) of the fit's beta.\n  # Uncomment the line below to apply variable selection\n\n  # avfosfit = avfosfit * (abs(avfosfit) >= 3 * C * rtilda)\n\n  return(list(\"coefficients\"=avfosfit,\n              \"betas\"=betas,\n              \"lambdas\"=rs,\n              \"optimIndex\"=statsIt))\n}\n",
    "created" : 1478630794233.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3101794522",
    "id" : "AA44DAAA",
    "lastKnownWriteTime" : 1478630898,
    "path" : "~/Documents/project/fos.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "type" : "r_source"
}